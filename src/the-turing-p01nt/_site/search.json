[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The SFI Centre for Research Training in Artificial Intelligence aims to create an internationally connected and globally recognised centre of excellence for the training of postgraduate students and the up-skilling of industry-based staff in key technical topics in artificial intelligence and data analytics."
  },
  {
    "objectID": "posts/issue04/index.html",
    "href": "posts/issue04/index.html",
    "title": "It’s Time To Talk About People Behind PhD",
    "section": "",
    "text": "Credit: Image created with Copilot by request “Give me image which the best displays PhD researchers working in the field of AI engineering”\n\n\nDeep Learnings: Challenges faced by non-EU PhD researchers applying for Travel Visas\nJanet Choi Sr. Research Co-ordinator, SFI CRT in Artificial Intelligence Programme Co-ordinator\nAs an advocate for PhD students in our programme, the task was to facilitate the process of obtaining appointments for 23 students who were to attend an event at the University of Amsterdam.  \nThe process was by no means difficult, at first, I resolved to the fact that it would be a simple phone call. “Just make a call to VFS Global in Dublin, ask for the Embassy of the Netherlands, request for a group appointment.’’ I said to myself. \nI was on hold, after waiting for the standard opening welcome and was put on to an operator who couldn’t manage my request, I was transferred to a case manager. Who was going to really help me now, to get my 23 appointments before April 2024. This was June, so loads of time at hand. \nI was wrong. \nI had a brief call with the case manager, who assured me that they could handle all 23 request in one day. I waited for the promised email, which came after the call to thank me for my phone call and dealing with VFS global. I waited, and waited for details any details of times and dates for the appointments. Days and weeks passed.\nAnother phone call attempted again to reach the same case manager who was so happy to facilitate me in our first phone call.  Not to be boring, but this resulted in more than 20+ phone calls by landline and mobile to gain clarity on the appointment dates and times. Between calls, I would receive email acknowledgement emails were digitally signed with a Dublin address, but in actuality the call centre was based in Mumbai. On several occasions, I had received multiple missed calls from unknown numbers, followed by emails from VFS Global stating that they tried to contact unsuccessfully which delayed the process by a day. A survey by email would promptly follow the initial email. \nLong story short, from all 23 appointments being able to manage in one day, the discussion was hijacked, and appointments reduced, down from all, to 6 per day, down from 6 to 4 per day and finally 2 per day as the days went on. A painful call came when a family appointment for 4 (which was to be processed altogether) was split to two individual appointments on two separate days. \nI felt sick with worry multiple times seeing students’ disappointment. This trip, the programme’s first international one, was too important to fail. I couldn’t let all the effort spent on meetings, discussions, and countless emails with Amsterdam AI and the PhDs be wasted. The time invested in securing flights and accommodation before applications even opened felt at risk.\n\n\nThe trips required the following documents, and in future I would advise all the students to do this, have a portfolio ready of all the documents required as listed below on their person when granted a visit to VFS Global. \n\n\n1) Agenda/Itinerary for the visit on visiting site Letterhead\n2) Letter of Invitation from the institution/conference/event\n3) Flights and accommodation booked.\n4) Letter of Support from their institution\n5) Travel Insurance (personal and institutional)\n6) Bank Statements\n7) 3 months’ pay slips\n8) Fully completed Application \n9) VFS Global email invitation\n10) Fee \n\n\nIn all 40 of us made the visit across to The Netherland’s and springboarded some to apply for their Schengen Visa. \n\n\nConsidering the current monopoly that VFS global have as the single operator for a majority of Embassies who rely on their service, I would say that their service is severely restrictive, and mediocre at best. \nFrom their webpage, they state leveraging a robust experience in visa application processing, providing government with a holistic administrative solution to processing passport applications and provision of efficient consular services. https://www.vfsglobal.com/en/individuals/about.html\nNot to mention, that if the appointment is early morning and one is travelling from Galway, Limerick or Cork the journey taken to make the appointment by car, train or bus is an additional time that needs to be factored in and in my view, if one cannot secure a visa appointment through their website, it could take 6-9 months to actually get one. Or contact that them directly, keeping in mind that you’ll need to persistently seek updates and clarification until a confirmed appointment time and date is provided."
  },
  {
    "objectID": "posts/issue02/index.html",
    "href": "posts/issue02/index.html",
    "title": "It’s Time To Talk About Bias",
    "section": "",
    "text": "Welcoming words\nWelcome to the second issue of the CRT-AI Newsletter “The Turing Point”. This time we talk about bias in AI. Even though the topic has received quite a lot of attention in recent years, the level of awareness remains low. Knowing what tools a scientist can use to spot, evaluate and mitigate any form of bias is of great importance. For this issue, our team prepared a list of interesting articles, tools and datasets that can help you on your PhD ​​​​​journey.\n\n\nAce Your PhD Life With Soft Skills \n\nSoft Skills Toolkit\n\n\nSoft Skills: What are they? \nSoft skills are core skills sought after in every profession. Which soft skills are relevant for your career? And which ones should you focus on developing?\nRead More \n\n\nCommunication with your supervisor\nYour supervisor has a vested interest in your success. Set the right tone and communication style when you meet with them.\nRead More \n\n\nWaiting for the motivation fairy\nIt’s easy to give in to procrastination — but Hugh Kearns and Maria Gardiner offer some tips for getting your drive back.\nRead More \n\n\nEscape Your Chair challenge\nEscape your chair by signing up to this challenge\nRead More \n\n\n\nFind Your CRT-AI Science Buddy\nFancy a collaboration with your CRT-AI mates? You have created that amazing tool but nobody knows about it? You have spent hundreds of hours gathering this incredible database but you are the only one using it? We have your back!\n\nThe CRT-AI’s Padlet now provides you with a nice place to advertise your work and find your AI soulmate &lt;3. Whether you are looking for a collaboration or have a nice database to share, you can publish a post with everything you want people to know. If it isn’t your case yet, remember to save this Padlet to your favorites; you could be surprised by how inspiring this mood board will soon be! Here we welcome every single one of our members and want everyone to feel comfortable. You can therefore post by yourself or by filling in our special form!\nFind your buddyor fill the form and we will do the job\n\nfeatured article\n\n\n\nBlueDot: AI Goes Viral\n\nby Yanlin Mi\nBefore the COVID-19 pandemic swept the globe, Artificial Intelligence (AI) was already being used in almost every industry. In the global fight against the COVID-19 pandemic, it has completely entered the human living space. A new generation of information technology, such as artificial intelligence, has improved human cognition and disease understanding in the fields of medical care, health, and public health.\nAI and big data predictive modeling are improving and becoming increasingly more accurate, implying more accurate disease spread projections, and more time to produce more effective treatments. \n\nBlueDot is a AI data-driven public health risk assessment firm that specializes in tracking, finding, and conceptualizing the spread of infectious illnesses. It accomplishes this by combining (AI) and Natural Language Processing (NLP) techniques to process data from a variety of sources, including national statistical reports from various regions, global news media, global airline ticket data, population density data, global infectious disease alerts, climate reports, and insect vector and animal disease repositories. These datasets are used to train AI models, which allow their systems to deliver warnings in real time and on a consistent basis. This makes it possible to conduct “automated infectious disease surveillance”.\n\nIndeed, BlueDot detected the COVID-19 pandemic in Wuhan on December 31, 2019, 6 days before the US Centers for Disease Control and Prevention (CDC) and 9 days before the World Health Organization (WHO) issued a warning. It was among the first in the world to identify the emerging risk from COVID-19 in Hubei province. \n\nIt is through the daily analysis of 100,000 articles in 65 languages with keywords related to pandemic diseases, animal diseases, public health, and global flight dynamics, which BlueDot uses to track the flow of infected populations by analysing a global database of airline tickets in order to predict the next cities to be infected and inform its clients of potential outbreaks and the spread of infectious diseases. \nYet, BlueDot’s application is not restricted to Covid-19, and six months before the official report in 2016, BlueDot had predicted the spread of Zika virus to Florida. In 2014, Bluedot  had also correctly predicted the danger of an Ebola virus outbreak in West Africa. BlueDot’s COVID Data Suite delivers bespoke, near-real-time intelligence to governments, hospitals and airlines, to track COVID-19’s movements, and states that the technology can also be used to track disorders including meningitis, yellow fever, and anthrax.\nRead more\n\n\n\nTuring Point Podcast\nThe inaugural Turing Point Podcast! Cathy Roche moderates a lively conversation with Dr. Dave Lewis and ​Dr. P.J. Wall about AI Ethics and how AI technology can have a positive impact!\n\n\nArtificial Intelligence Dictionary\n\nComing to your AID (AI Dictionary)\n\nby ​Cathy Roche\n\n\n\n\n\n\n\n\n\n\nArtificial Intelligence\n\n/ɑː(ɹ)təˈfɪʃəl//ɪnˈtelɪdʒəns/\nAs a term, Artificial Intelligence (AI), refers to computer systems that perform functions and tasks, such as facial recognition, once thought to be exclusive abilities of living, intellectual beings.  Computer systems can be designed to achieve goals similar to human activity (e.g. learning and reasoning). AI can also be optimised to exceed human competencies, including identifying variables that have the greatest impact on an outcome. While the definition of AI has endured over time, examples of such technology have varied as the ability of computer systems to simulate human behaviour and thinking has progressed. A computer’s ability to play chess was once considered an example of AI, now Artificial Intelligence is used to describe smart assistants and self-driving cars.\n\n\n\nMachine Learning\n\n/məˈʃiːn/ /ˈlɜːnɪŋ/\nA branch of Artificial Intelligence, Machine Learning (ML) is a technique based on systems learning from data. ML describes how a computer system teaches itself to identify patterns within the data and can make decisions with little human input. As the machine learns automatically from past data, there is no need for explicit programming. Through ML, data is analysed through the automated building of analytical models: the system adapts its algorithms to improve the accuracy of pattern detection simply based on the data observed and using its own statistical tools. Just as humans make decisions based on our previous experience, ML enables computers to keep learning and adapting by inputting new data. For example, look at how humans learn about dogs. Our initial dataset may include seeing family pets, dogs on TV or looking at pictures online. From this dataset, we feel we are able to identify whether future animals with a tail, four legs and of a certain size are in fact dogs. What happens when we get data about dogs that is different from our initial dataset e.g. a fox is not a domestic dog?  What we do is refine our thinking about dogs so that in future, we will be able to determine more accurately what is a dog when we see dogs of a different height and weight.\n\n\n\nNeural Network\n\n /ˈnjʊər(ə)l/ /ˈnɛtwəːk/\nStructured in a way similar to how neurons are organised in the brain, artificial neural networks (or neural networks) are a set of algorithms designed to recognise patterns. Consisting of layers of nodes,  each designed to behave in a way comparable to a neuron, neural networks are used in Deep Learning.  The first layer is the input layer, then there are hidden layers and lastly, the output layer.  Each node performs a calculation and this is then passed to other nodes in the neural network. Within this model, the neurons (which are really perceptrons) receive the input, apply an assigned weight and as the model is trained, these weights can be adjusted to make the model more accurate. Neural networks can be used to classify and cluster and there are several types, such as recurrent or convolutional. The usability of a certain type of neural network depends on the task or application being undertaken.  \n\n\n\nBias\n\n/ˈbʌɪəs/\nAI systems use machine learning algorithms to perform their function. These algorithms train on data and can often produce results which reflect erroneous assumptions in that data. This is because the data has human prejudices, both conscious and unconscious, encoded within it. Bias can take many forms, such as gender discrimination, ageism or racial prejudice. While AI systems reproduce human biases, they also scale them: perpetuating bias in decision-making to a level that could constitute algorithmic discrimination. An AI is considered biased if the decisions it makes penalise or reward groups of people for reasons that are prejudiced.AI bias can stem from several sources, including\n\nLow quality of the data used for training models. For example, by using an AI-based recruiting tool in a predominantly male company or industry. Training on historical employee data, it is likely that the AI tool will replicate gender bias.\nHow training data is collected and or processed. For example, a data scientist may exclude important entries or end up under- or over-sampling which can penalise minority classes.\nDatasets that are not robust.\nFor example, a dataset which fails to distinguish between different groups could lead to decisions which treat people in a uniform way, without considering important differences.\nWeak model validation. For example, a model may perform very well on the training data but not be generalisable.\nImplicit bias: For example, where human biases are already encoded in the datasets.\n\n\n\n\nEthics Washing\n\n/ˈɛθɪks/ /ˈwɒʃɪŋ/\nAlso known as ethics shopping or ethics theatre, ethics washing refers to when an organisation pays lip service to ethics to signal to the public, shareholders and policy-makers that it is committed to responsible AI development but does little to ensure it happens in practice. In this way, an organisation can appear to take AI ethics seriously but in reality it is window dressing. The ethical policies and frameworks adopted are mainly for show and do not create any real accountability or obligation on the organisation.  Ethics washing is used to avoid external regulation and official scrutiny by appearing to have voluntarily adopted broad ethical obligations. The problem of ethics washing is that it masks whether organisations are actually making efforts towards developing ethical AI systems.\n\n\nCOMPETITION\n\n\n\n\nLOL My Thesis\nTired of that too serious life? Take a step back and tell us about that laughable work you’ve been exhausting yourself on! Here it can be about your thesis, a project you have been working on, your Master’s or Bachelor’s dissertation, etc. It does not matter. All we want you to do is to provide us with a new title that highlights the absurdity of your work.https://lolmythesis.com/ is a rich bank of hilarious examples, and we might have stolen their idea. Yet, we know that misery does not only affect others, and now we want to laugh at our CRT-AI community. Come up with your best line and submit it here! The best ones will receive a chocolate bar and be featured in our next issue!\nSend your entry\nsee examples\n\n\nHistory of Neural Networks\n\nby Sharmi Dev Gupta and Lavanya ​Vinod Pampana\nIt all starts with a fundamental question, “do machines think?” rather, “can we make machines think?”. If we could replicate the human brain in the best possible way, we might just achieve it. Make the impossible possible. The next question that entails in replication is more philosophical, “what is intelligence”? Is it innate to living organisms or is it something that can be synthesized? \nRead more\nIn this quest for making machines think, the neurophysiologist Warren McCullough and mathematician Walter Pitts in 1943 worked together to understand ‘how neurons work’. They modeled a simple neural network using electrical circuits. This was the first step in the invention of an artificial neuron. The model’s simplicity was a major limitation since it only accepted binary inputs, incorporated threshold step activation functions and it did not account weights. In 1949, Donald Hebb proposed when two neurons fire together then the connection between two neurons is strengthened. It was ascertained that this is one of fundamental operations for learning and memory.  In the 1950s, Nathanial Rochester, from IBM Research Laboratories, tried to simulate a neural network on IBM 704. He was the chief architect of IBM 701 computers. His group was assigned to pattern recognition, information theory and so on. In 1956, Nathaniel Rochester (IBM Laboratories) along with John McCarthy (Dartmouth College), Claude E.Shannon (Bell Telephone Laboratories) and Marvin L. Minsky (Harvard University) formed a working group and submitted a proposal for a workshop/conference to the Rockefeller Foundation.\nThis workshop took place in July, August 1956 and is recognized as the official birthplace of Artificial Intelligence. Attendees came from diverse backgrounds: electrical engineering, psychology, mathematics and more. The proposal for the workshop is shared below, which gives a broader definition of Artificial Intelligence:\n“An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. … For the present purpose the artificial intelligence problem is taken to be that of making a machine behave in ways that would be called intelligent if a human were so behaving.” [9]\n“The attendees at the 1956 Dartmouth conference shared a common defining belief, namely that the act of thinking is not something unique either to humans or indeed even biological beings. Rather, they believed that computation is a formally deducible phenomenon which can\nbe understood in a scientific way and that the best nonhuman instrument for doing so is the digital computer (McCormack, 1979).” [9]\n\nMarvin Minsky, Claude Shannon, Ray Solomonoff and other scientists at the Dartmouth Summer Research Project on Artificial Intelligence (Photo: Margaret Minsky) [9]\nAn image of the perceptron from Rosenblatt’s “The Design of an Intelligent Automaton,” Summer 1958 [8]\nIt was period when everyone was trying to implement a neural network for a real-time application. Though the traditional Von Neumann architecture dominated the computing scene then, John Von Neumann worked on imitating neural functions by using telegraph relays or vacuum tube in 1957 and was not quite successful. Frank Rosenblatt developed the first perceptron by altering McCulloch-Pitt’s neuron. This perceptron follows the Hebbe’s rule, that is weighting the inputs, and was the building block of formation of neural networks. In July 1958, IBM 704, a 5-ton computer, the size of a room was fed a series of punch cards. The computer taught itself to distinguish the cards marked on the left from the cards marked on the right in 50 trials. This was the first demonstration of perceptron, the neural machine capable of delivering and conceiving the original idea which is still in use. He discussed the perceptron in detail in his 1962 book, Principles of Neurodynamics.\nAround the same time, in 1959, Bernard Widrow and Marcian Hoff (Stanford) developed models called ADALINE and MADALINE. For Stanford’s love of acronyms, the names mean Multiple ADAptive LINear Elements. ADALINE could recognize binary patterns, if it was reading streaming bits from a phone line, it could predict the next one. MADALINE was the first neural network applied to real world problems that eliminates the echoes on phone lines using an adaptive filter. Though the system is as ancient as air traffic control systems, it is still in use. \nMarvin Minsky and Seymour Papert in 1969, proved that the perceptron was limited in their book Perceptrons. At conferences, Minsky and Rosenblatt publicly debated the viability of perceptron. \n“Rosenblatt had a vision that he could make computers see and understand language. And Marvin Minsky pointed out that that’s not going to happen, because the functions are just too simple.” [8]\nThe problem with Rosenblatt’s perceptron was it only had one layer and while modern networks have many layers. The real contending problem was that the model provided provision for generating AND, OR, NAND and NOR gates but could not generate the XOR gate. This led to a winter period in neural networks where research progress halted in this direction for a few years. \nThe thawing of this frosty AI winter began in 1982 when Jon Hopfield published what is known today as the HopfieldNet at the NAS(National Academy of Sciences). Around the same time, the field got its much needed competitive boost when Japan announced its 5th  generation effort on the research of Neural networks. This helped US research institutes pitch for larger amounts of funding and in 1985 the American Institute of Physicals established  the Neural Networks for Computing annual meeting , followed by the Institute of Electrical and Electronics Engineers (IEEE) in 1987.\nThe year 1997 was another milestone moment. A recurrent neural network framework , LSTM was proposed by Schmidhuber and Hochrelter. They introduced the Constant Error Carousel units, to deal with the vanishing gradient problem. \nIn 1998, Yann LeCun published a seminal paper on the Gradient - Based Learning applied to document recognition.  Gradient based learning provided the framework to build systems, which cater to learning architectures that can handle high dimensions of input, high degree of variability and the complex non linear relationships between the inputs and outputs.\nSummaries of some of the important academic  papers are discussed below: \n\n\nAttention Is All You Need\n\n\nThe authors propose a simple network architecture called the Transformer based on attention mechanisms, dispensing the need of using complex recurrent networks or CNNs by relying entirely on self attention. The paper describes the architecture of the model with an encoder decoder structure where each encoder has sublayers, one a multihead self attention mechanism and another a fully connected feed forward network. The decoder is composed of a stack of 6 identical layers where the sublayer performs multi head attention over the output of the encoder stack. The application of attention is done in 3 different ways:\n\nIn the “encoder-decoder” attention layers mimic the typical encoder-decoder attention mechanism in sequence to sequence models.\nThe encoder contains self attention layers where all the keys,values and queries come from the output of the previous layer in the encoder. All the positions in the encoder can attend to all the positions in the previous layer of the encoder.\nThe self attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder. \n\nThe paper compares the self attention model to the recurrent and convolutional networks and comes to the conclusion that self attention layers are faster than recurrent layers in terms of computational complexity. The complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer i.e. the approach taken in the paper. \n This paper achieves a new state of the art when it comes to translational tasks. \nThe code is available at https://github.com/tensorflow/tensor2tensor.\n\n\n 2. Bandwidth Prediction and Congestion Control for ABR Traffic Based on Neural Networks\nThe paper uses Back propagation Neural Networks for congestion control in ATM networks for predicting the bursty available bandwidth for ABR traffic effectively and to force the queue level in the Buffer to a desired region. The fairness of this model is achieved through the fair algorithm. \n\n\n 3. Using Artificial Neural Network Modeling in Forecasting Revenue: Case Study in National Insurance Company/Iraq\nThe paper aims to forecast the insurance premiums revenue of the National Insurance Company between the years 2012 to 2053 using Artificial Neural Network based on the annual data available for the insurance premiums revenue between 1970 to 2011. The authors used a Neural network fitting tool to help select the data, create and train a network and evaluate its performance.The approach is used by taking the annual investment income of National Insurance Company as an independent (Input) variable. The experiments show that the best architecture for fitting a neural network as one input vector, five hidden layers and the output is one vector (1-5-1), the ratio of increasing the insurance premiums revenue is approximately 120% for the period between 2012 and 2053.\n\n\n 4. Neural Network Approach to Forecast the State of the Internet of Things Elements\nThe aim of this paper is to use neural networks to predict the states of the elements present in an IOT based architecture. The proposed model/ architecture of the neural networks is a combination of multilayered perpetron and probabilistic neural networks. Authors analyze the performance of this model based on accuracy and efficiency of the model. The combined ANN network helps realize a forecasting and monitoring model for the Internet of Things. This results in reduction of IOT administration costs and emergency resolutions. \n\n\n 5. AnyNets: Adaptive Deep Neural Networks for medical data with missing values\nThe paper introduces a novel class of adaptive deep neural networks called AnyNets which is designed to remove the need for imputation for missing data for patient records in medicine. This is because a large number of patient records contain incomplete information and measurements which are then filled up using default values which causes bias and limits generalization. The paper goes on to process various kinds of input values through the medical datasets under both supervised and unsupervised learning and achieve better results than the electronic medical records or registry.\n\n\nReferences:\n\nhttps://home.csulb.edu/~cwallis/artificialn/History.htm#:~:text=One%20of%20the%20difficulties%20with,incorporate%20weighting%20the%20different%20inputs.\nhttps://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/History/history1.html\nhttps://medium.com/analytics-vidhya/brief-history-of-neural-networks-44c2bf72eec\nhttps://towardsdatascience.com/a-concise-history-of-neural-networks-2070655d3fec\nhttps://blogs.umass.edu/comphon/2017/06/15/did-frank-rosenblatt-invent-deep-learning-in-1962/\nhttps://home.csulb.edu/~cwallis/artificialn/History.htm#:~:text=One%20of%20the%20difficulties%20with,incorporate%20weighting%20the%20different%20inputs.\nhttps://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/History/history1.html\nhttps://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon\nhttps://www.cantorsparadise.com/the-birthplace-of-ai-9ab7d4e5fb00\nhttps://medium.com/analytics-vidhya/understanding-basics-of-deep-learning-by-solving-xor-problem-cb3ff6a18a06\nhttps://www.forbes.com/sites/gilpress/2017/08/27/artificial-intelligence-ai-defined/?sh=55526a987661\nhttps://developer.ibm.com/articles/cc-cognitive-neural-networks-deep-dive/"
  },
  {
    "objectID": "posts/issue01/index.html",
    "href": "posts/issue01/index.html",
    "title": "The Turing Point",
    "section": "",
    "text": "Professor O’Sullivan ​Director of The Centre for Research Training in Artificial Intelligence\n\n\n\nDirector’s Message \nWelcome to the first edition of the CRT-AI Newsletter. It is our pleasure to announce, this is The Turing Point. \nOur goal is to keep you informed of our latest developments and news and provide you with valuable information during your PhD journey. \nCRT-AI’s newsletter committee is comprised of students across cohorts one through three. There are six committee members, and the advisory committee is composed of the six Co-Directors of the CRT-AI.\nWhether people realize it or not, AI is now part of their daily lives, not a piece of fiction. A number of industries have become prominent for adopting artificial intelligence, including high tech and telecommunications, financial services, and healthcare. \nThe growth of AI Corporate Investment worldwide, due to Covid-19 Pandemic, has accelerated in areas such as biotech, and the biggest increase has been in healthcare and pharma, and it appears that AI hiring, investment, and adoption have increased globally.\nThe training of PhDs in this area is necessary to better meet the needs of the sector, improve efficiencies, and improve the future lives of millions of people. \nLast but not least, spring is approaching, and we all will be able to get up and moving again with plenty of fresh air around after a long period of hibernation. As you pursue your PhD, I wish you good health, happiness, and success. \nPlease contact the newsletter committee through the CRT-AI Slack workspace if you would like to donate your time. \n\n\nAce Your PhD Life With Soft Skills \n\nSoft Skills Toolkit\n\n\nSoft Skills: What are they? \nSoft skills are core skills sought after in every profession. Which soft skills are relevant for your career? And which ones should you focus on developing?\nRead More \n\n\nCommunication with your supervisor\nYour supervisor has a vested interest in your success. Set the right tone and communication style when you meet with them.\nRead More \n\n\nWaiting for the motivation fairy\nIt’s easy to give in to procrastination — but Hugh Kearns and Maria Gardiner offer some tips for getting your drive back.\nRead More \n\n\nEscape Your Chair challenge\nEscape your chair by signing up to this challenge\nRead More \n\n\n\nFind Your CRT-AI Science Buddy\nFancy a collaboration with your CRT-AI mates? You have created that amazing tool but nobody knows about it? You have spent hundreds of hours gathering this incredible database but you are the only one using it? We have your back!\n\nThe CRT-AI’s Padlet now provides you with a nice place to advertise your work and find your AI soulmate &lt;3. Whether you are looking for a collaboration or have a nice database to share, you can publish a post with everything you want people to know. If it isn’t your case yet, remember to save this Padlet to your favorites; you could be surprised by how inspiring this mood board will soon be! Here we welcome every single one of our members and want everyone to feel comfortable. You can therefore post by yourself or by filling in our special form!\nFind your buddyor fill the form and we will do the job\n\nfeatured article\n\n\n\nBlueDot: AI Goes Viral\n\nby Yanlin Mi\nBefore the COVID-19 pandemic swept the globe, Artificial Intelligence (AI) was already being used in almost every industry. In the global fight against the COVID-19 pandemic, it has completely entered the human living space. A new generation of information technology, such as artificial intelligence, has improved human cognition and disease understanding in the fields of medical care, health, and public health.\nAI and big data predictive modeling are improving and becoming increasingly more accurate, implying more accurate disease spread projections, and more time to produce more effective treatments. \n\nBlueDot is a AI data-driven public health risk assessment firm that specializes in tracking, finding, and conceptualizing the spread of infectious illnesses. It accomplishes this by combining (AI) and Natural Language Processing (NLP) techniques to process data from a variety of sources, including national statistical reports from various regions, global news media, global airline ticket data, population density data, global infectious disease alerts, climate reports, and insect vector and animal disease repositories. These datasets are used to train AI models, which allow their systems to deliver warnings in real time and on a consistent basis. This makes it possible to conduct “automated infectious disease surveillance”.\n\nIndeed, BlueDot detected the COVID-19 pandemic in Wuhan on December 31, 2019, 6 days before the US Centers for Disease Control and Prevention (CDC) and 9 days before the World Health Organization (WHO) issued a warning. It was among the first in the world to identify the emerging risk from COVID-19 in Hubei province. \n\nIt is through the daily analysis of 100,000 articles in 65 languages with keywords related to pandemic diseases, animal diseases, public health, and global flight dynamics, which BlueDot uses to track the flow of infected populations by analysing a global database of airline tickets in order to predict the next cities to be infected and inform its clients of potential outbreaks and the spread of infectious diseases. \nYet, BlueDot’s application is not restricted to Covid-19, and six months before the official report in 2016, BlueDot had predicted the spread of Zika virus to Florida. In 2014, Bluedot  had also correctly predicted the danger of an Ebola virus outbreak in West Africa. BlueDot’s COVID Data Suite delivers bespoke, near-real-time intelligence to governments, hospitals and airlines, to track COVID-19’s movements, and states that the technology can also be used to track disorders including meningitis, yellow fever, and anthrax.\nRead more\n\n\n\nTuring Point Podcast\nThe inaugural Turing Point Podcast! Cathy Roche moderates a lively conversation with Dr. Dave Lewis and ​Dr. P.J. Wall about AI Ethics and how AI technology can have a positive impact!\n\n\nArtificial Intelligence Dictionary\n\nComing to your AID (AI Dictionary)\n\nby ​Cathy Roche\n\n\n\n\n\n\n\n\n\n\nArtificial Intelligence\n\n/ɑː(ɹ)təˈfɪʃəl//ɪnˈtelɪdʒəns/\nAs a term, Artificial Intelligence (AI), refers to computer systems that perform functions and tasks, such as facial recognition, once thought to be exclusive abilities of living, intellectual beings.  Computer systems can be designed to achieve goals similar to human activity (e.g. learning and reasoning). AI can also be optimised to exceed human competencies, including identifying variables that have the greatest impact on an outcome. While the definition of AI has endured over time, examples of such technology have varied as the ability of computer systems to simulate human behaviour and thinking has progressed. A computer’s ability to play chess was once considered an example of AI, now Artificial Intelligence is used to describe smart assistants and self-driving cars.\n\n\n\nMachine Learning\n\n/məˈʃiːn/ /ˈlɜːnɪŋ/\nA branch of Artificial Intelligence, Machine Learning (ML) is a technique based on systems learning from data. ML describes how a computer system teaches itself to identify patterns within the data and can make decisions with little human input. As the machine learns automatically from past data, there is no need for explicit programming. Through ML, data is analysed through the automated building of analytical models: the system adapts its algorithms to improve the accuracy of pattern detection simply based on the data observed and using its own statistical tools. Just as humans make decisions based on our previous experience, ML enables computers to keep learning and adapting by inputting new data. For example, look at how humans learn about dogs. Our initial dataset may include seeing family pets, dogs on TV or looking at pictures online. From this dataset, we feel we are able to identify whether future animals with a tail, four legs and of a certain size are in fact dogs. What happens when we get data about dogs that is different from our initial dataset e.g. a fox is not a domestic dog?  What we do is refine our thinking about dogs so that in future, we will be able to determine more accurately what is a dog when we see dogs of a different height and weight.\n\n\n\nNeural Network\n\n /ˈnjʊər(ə)l/ /ˈnɛtwəːk/\nStructured in a way similar to how neurons are organised in the brain, artificial neural networks (or neural networks) are a set of algorithms designed to recognise patterns. Consisting of layers of nodes,  each designed to behave in a way comparable to a neuron, neural networks are used in Deep Learning.  The first layer is the input layer, then there are hidden layers and lastly, the output layer.  Each node performs a calculation and this is then passed to other nodes in the neural network. Within this model, the neurons (which are really perceptrons) receive the input, apply an assigned weight and as the model is trained, these weights can be adjusted to make the model more accurate. Neural networks can be used to classify and cluster and there are several types, such as recurrent or convolutional. The usability of a certain type of neural network depends on the task or application being undertaken.  \n\n\n\nBias\n\n/ˈbʌɪəs/\nAI systems use machine learning algorithms to perform their function. These algorithms train on data and can often produce results which reflect erroneous assumptions in that data. This is because the data has human prejudices, both conscious and unconscious, encoded within it. Bias can take many forms, such as gender discrimination, ageism or racial prejudice. While AI systems reproduce human biases, they also scale them: perpetuating bias in decision-making to a level that could constitute algorithmic discrimination. An AI is considered biased if the decisions it makes penalise or reward groups of people for reasons that are prejudiced.AI bias can stem from several sources, including\n\nLow quality of the data used for training models. For example, by using an AI-based recruiting tool in a predominantly male company or industry. Training on historical employee data, it is likely that the AI tool will replicate gender bias.\nHow training data is collected and or processed. For example, a data scientist may exclude important entries or end up under- or over-sampling which can penalise minority classes.\nDatasets that are not robust.\nFor example, a dataset which fails to distinguish between different groups could lead to decisions which treat people in a uniform way, without considering important differences.\nWeak model validation. For example, a model may perform very well on the training data but not be generalisable.\nImplicit bias: For example, where human biases are already encoded in the datasets.\n\n\n\n\nEthics Washing\n\n/ˈɛθɪks/ /ˈwɒʃɪŋ/\nAlso known as ethics shopping or ethics theatre, ethics washing refers to when an organisation pays lip service to ethics to signal to the public, shareholders and policy-makers that it is committed to responsible AI development but does little to ensure it happens in practice. In this way, an organisation can appear to take AI ethics seriously but in reality it is window dressing. The ethical policies and frameworks adopted are mainly for show and do not create any real accountability or obligation on the organisation.  Ethics washing is used to avoid external regulation and official scrutiny by appearing to have voluntarily adopted broad ethical obligations. The problem of ethics washing is that it masks whether organisations are actually making efforts towards developing ethical AI systems.\n\n\nCOMPETITION\n\n\n\n\nLOL My Thesis\nTired of that too serious life? Take a step back and tell us about that laughable work you’ve been exhausting yourself on! Here it can be about your thesis, a project you have been working on, your Master’s or Bachelor’s dissertation, etc. It does not matter. All we want you to do is to provide us with a new title that highlights the absurdity of your work.https://lolmythesis.com/ is a rich bank of hilarious examples, and we might have stolen their idea. Yet, we know that misery does not only affect others, and now we want to laugh at our CRT-AI community. Come up with your best line and submit it here! The best ones will receive a chocolate bar and be featured in our next issue!\nSend your entry\nsee examples\n\n\nHistory of Neural Networks\n\nby Sharmi Dev Gupta and Lavanya ​Vinod Pampana\nIt all starts with a fundamental question, “do machines think?” rather, “can we make machines think?”. If we could replicate the human brain in the best possible way, we might just achieve it. Make the impossible possible. The next question that entails in replication is more philosophical, “what is intelligence”? Is it innate to living organisms or is it something that can be synthesized? \nRead more\nIn this quest for making machines think, the neurophysiologist Warren McCullough and mathematician Walter Pitts in 1943 worked together to understand ‘how neurons work’. They modeled a simple neural network using electrical circuits. This was the first step in the invention of an artificial neuron. The model’s simplicity was a major limitation since it only accepted binary inputs, incorporated threshold step activation functions and it did not account weights. In 1949, Donald Hebb proposed when two neurons fire together then the connection between two neurons is strengthened. It was ascertained that this is one of fundamental operations for learning and memory.  In the 1950s, Nathanial Rochester, from IBM Research Laboratories, tried to simulate a neural network on IBM 704. He was the chief architect of IBM 701 computers. His group was assigned to pattern recognition, information theory and so on. In 1956, Nathaniel Rochester (IBM Laboratories) along with John McCarthy (Dartmouth College), Claude E.Shannon (Bell Telephone Laboratories) and Marvin L. Minsky (Harvard University) formed a working group and submitted a proposal for a workshop/conference to the Rockefeller Foundation.\nThis workshop took place in July, August 1956 and is recognized as the official birthplace of Artificial Intelligence. Attendees came from diverse backgrounds: electrical engineering, psychology, mathematics and more. The proposal for the workshop is shared below, which gives a broader definition of Artificial Intelligence:\n“An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. … For the present purpose the artificial intelligence problem is taken to be that of making a machine behave in ways that would be called intelligent if a human were so behaving.” [9]\n“The attendees at the 1956 Dartmouth conference shared a common defining belief, namely that the act of thinking is not something unique either to humans or indeed even biological beings. Rather, they believed that computation is a formally deducible phenomenon which can\nbe understood in a scientific way and that the best nonhuman instrument for doing so is the digital computer (McCormack, 1979).” [9]\n\nMarvin Minsky, Claude Shannon, Ray Solomonoff and other scientists at the Dartmouth Summer Research Project on Artificial Intelligence (Photo: Margaret Minsky) [9]\nAn image of the perceptron from Rosenblatt’s “The Design of an Intelligent Automaton,” Summer 1958 [8]\nIt was period when everyone was trying to implement a neural network for a real-time application. Though the traditional Von Neumann architecture dominated the computing scene then, John Von Neumann worked on imitating neural functions by using telegraph relays or vacuum tube in 1957 and was not quite successful. Frank Rosenblatt developed the first perceptron by altering McCulloch-Pitt’s neuron. This perceptron follows the Hebbe’s rule, that is weighting the inputs, and was the building block of formation of neural networks. In July 1958, IBM 704, a 5-ton computer, the size of a room was fed a series of punch cards. The computer taught itself to distinguish the cards marked on the left from the cards marked on the right in 50 trials. This was the first demonstration of perceptron, the neural machine capable of delivering and conceiving the original idea which is still in use. He discussed the perceptron in detail in his 1962 book, Principles of Neurodynamics.\nAround the same time, in 1959, Bernard Widrow and Marcian Hoff (Stanford) developed models called ADALINE and MADALINE. For Stanford’s love of acronyms, the names mean Multiple ADAptive LINear Elements. ADALINE could recognize binary patterns, if it was reading streaming bits from a phone line, it could predict the next one. MADALINE was the first neural network applied to real world problems that eliminates the echoes on phone lines using an adaptive filter. Though the system is as ancient as air traffic control systems, it is still in use. \nMarvin Minsky and Seymour Papert in 1969, proved that the perceptron was limited in their book Perceptrons. At conferences, Minsky and Rosenblatt publicly debated the viability of perceptron. \n“Rosenblatt had a vision that he could make computers see and understand language. And Marvin Minsky pointed out that that’s not going to happen, because the functions are just too simple.” [8]\nThe problem with Rosenblatt’s perceptron was it only had one layer and while modern networks have many layers. The real contending problem was that the model provided provision for generating AND, OR, NAND and NOR gates but could not generate the XOR gate. This led to a winter period in neural networks where research progress halted in this direction for a few years. \nThe thawing of this frosty AI winter began in 1982 when Jon Hopfield published what is known today as the HopfieldNet at the NAS(National Academy of Sciences). Around the same time, the field got its much needed competitive boost when Japan announced its 5th  generation effort on the research of Neural networks. This helped US research institutes pitch for larger amounts of funding and in 1985 the American Institute of Physicals established  the Neural Networks for Computing annual meeting , followed by the Institute of Electrical and Electronics Engineers (IEEE) in 1987.\nThe year 1997 was another milestone moment. A recurrent neural network framework , LSTM was proposed by Schmidhuber and Hochrelter. They introduced the Constant Error Carousel units, to deal with the vanishing gradient problem. \nIn 1998, Yann LeCun published a seminal paper on the Gradient - Based Learning applied to document recognition.  Gradient based learning provided the framework to build systems, which cater to learning architectures that can handle high dimensions of input, high degree of variability and the complex non linear relationships between the inputs and outputs.\nSummaries of some of the important academic  papers are discussed below: \n\n\nAttention Is All You Need\n\n\nThe authors propose a simple network architecture called the Transformer based on attention mechanisms, dispensing the need of using complex recurrent networks or CNNs by relying entirely on self attention. The paper describes the architecture of the model with an encoder decoder structure where each encoder has sublayers, one a multihead self attention mechanism and another a fully connected feed forward network. The decoder is composed of a stack of 6 identical layers where the sublayer performs multi head attention over the output of the encoder stack. The application of attention is done in 3 different ways:\n\nIn the “encoder-decoder” attention layers mimic the typical encoder-decoder attention mechanism in sequence to sequence models.\nThe encoder contains self attention layers where all the keys,values and queries come from the output of the previous layer in the encoder. All the positions in the encoder can attend to all the positions in the previous layer of the encoder.\nThe self attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder. \n\nThe paper compares the self attention model to the recurrent and convolutional networks and comes to the conclusion that self attention layers are faster than recurrent layers in terms of computational complexity. The complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer i.e. the approach taken in the paper. \n This paper achieves a new state of the art when it comes to translational tasks. \nThe code is available at https://github.com/tensorflow/tensor2tensor.\n\n\n 2. Bandwidth Prediction and Congestion Control for ABR Traffic Based on Neural Networks\nThe paper uses Back propagation Neural Networks for congestion control in ATM networks for predicting the bursty available bandwidth for ABR traffic effectively and to force the queue level in the Buffer to a desired region. The fairness of this model is achieved through the fair algorithm. \n\n\n 3. Using Artificial Neural Network Modeling in Forecasting Revenue: Case Study in National Insurance Company/Iraq\nThe paper aims to forecast the insurance premiums revenue of the National Insurance Company between the years 2012 to 2053 using Artificial Neural Network based on the annual data available for the insurance premiums revenue between 1970 to 2011. The authors used a Neural network fitting tool to help select the data, create and train a network and evaluate its performance.The approach is used by taking the annual investment income of National Insurance Company as an independent (Input) variable. The experiments show that the best architecture for fitting a neural network as one input vector, five hidden layers and the output is one vector (1-5-1), the ratio of increasing the insurance premiums revenue is approximately 120% for the period between 2012 and 2053.\n\n\n 4. Neural Network Approach to Forecast the State of the Internet of Things Elements\nThe aim of this paper is to use neural networks to predict the states of the elements present in an IOT based architecture. The proposed model/ architecture of the neural networks is a combination of multilayered perpetron and probabilistic neural networks. Authors analyze the performance of this model based on accuracy and efficiency of the model. The combined ANN network helps realize a forecasting and monitoring model for the Internet of Things. This results in reduction of IOT administration costs and emergency resolutions. \n\n\n 5. AnyNets: Adaptive Deep Neural Networks for medical data with missing values\nThe paper introduces a novel class of adaptive deep neural networks called AnyNets which is designed to remove the need for imputation for missing data for patient records in medicine. This is because a large number of patient records contain incomplete information and measurements which are then filled up using default values which causes bias and limits generalization. The paper goes on to process various kinds of input values through the medical datasets under both supervised and unsupervised learning and achieve better results than the electronic medical records or registry.\n\n\nReferences:\n\nhttps://home.csulb.edu/~cwallis/artificialn/History.htm#:~:text=One%20of%20the%20difficulties%20with,incorporate%20weighting%20the%20different%20inputs.\nhttps://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/History/history1.html\nhttps://medium.com/analytics-vidhya/brief-history-of-neural-networks-44c2bf72eec\nhttps://towardsdatascience.com/a-concise-history-of-neural-networks-2070655d3fec\nhttps://blogs.umass.edu/comphon/2017/06/15/did-frank-rosenblatt-invent-deep-learning-in-1962/\nhttps://home.csulb.edu/~cwallis/artificialn/History.htm#:~:text=One%20of%20the%20difficulties%20with,incorporate%20weighting%20the%20different%20inputs.\nhttps://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/History/history1.html\nhttps://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon\nhttps://www.cantorsparadise.com/the-birthplace-of-ai-9ab7d4e5fb00\nhttps://medium.com/analytics-vidhya/understanding-basics-of-deep-learning-by-solving-xor-problem-cb3ff6a18a06\nhttps://www.forbes.com/sites/gilpress/2017/08/27/artificial-intelligence-ai-defined/?sh=55526a987661\nhttps://developer.ibm.com/articles/cc-cognitive-neural-networks-deep-dive/"
  },
  {
    "objectID": "posts/issue03/index.html",
    "href": "posts/issue03/index.html",
    "title": "It’s Time To Talk Generative AI",
    "section": "",
    "text": "Credit: Image created with Copilot by request “Give me image which the best displays Generative AI”\n\n\n\nWelcome to the 3rd Edition of the Turing Point Newsletter!\nGreetings from the CRT in AI Newsletter Committee and the release of the third edition of The Turing Point Newsletter.\nThis issue features our third podcast in the series, where we discuss the topic of Generative AI.\nSome might say the tool is flawed and unrealistic.\nAccording to a recent article published in The Harvard Business Review on November 8, 2023, they state that the model overestimates probable events while underestimating probable ones, failing to capture the true essence of human nature. This phenomenon is due to the tool’s endless production of synthetic data from its own outputs. So, has generative AI peaked? What is all the hype about?\nIt’s crucial, while we continue to explore these unexplored territories, to keep an open mind, but also exercise critical thought even though its future may appear dangerous. \nWe are nevertheless fascinated by the potential of Generative AI.\nFind out more from your fellow PhD researchers by tuning into our student-led AI podcast series on Generative AI and tell us what you think! \n\n\nGenerative AI Podcast\n\nEpisode 3:  Generative AI Podcast! CRT-AI PhD researcher Sharmi Dev Gupta who moderates a lively conversation with Rory Ward, CRT-AI student  about the impacts of Generative AI in this series of the Turing Point Podcasts in Artificial Intelligence.\n\n\nThe SFI CRT in AI PhD Handbook 2023/24\n\n\n\n2023 Events\n\n\n2024 Events"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "THE TURING P01NT",
    "section": "",
    "text": "It’s Time To Talk About People Behind PhD\n\n\nIssue #4\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMay 17, 2024\n\n\nJosh McGiff, Anaïs Murat, Cathy Roche, Janet Choi, Lavanya Vinod Pampana, Sharmi Dev Gupta, Yanlin Mi, Naa Korkoi Addo, Kislay Raj\n\n\n\n\n\n\n\n\n\n\n\n\nIt’s Time To Talk Generative AI\n\n\nIssue #3\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nDec 24, 2023\n\n\nAnaïs Murat, Mikhail Kudriavtsev, Sharmi Dev Gupta, Lavanya Vinod Pampana, Cathy Roche, Kislay Raj, Naa Korkoi Addo, Fitria Wulandari Ramlan, Yanlin Mi, Kevlyn Kadamala, Oleksii Dovhaniuk, Janet Choi\n\n\n\n\n\n\n\n\n\n\n\n\nIt’s Time To Talk About Bias\n\n\nIssue #2\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nAnaïs Murat, Cathy Roche, Janet Choi, Lavanya Vinod Pampana, Sharmi Dev Gupta, Yanlin Mi, Naa Korkoi Addo, Kislay Raj\n\n\n\n\n\n\n\n\n\n\n\n\nThe Turing Point\n\n\nIssue #1\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\nhistory\n\n\n\n\n\n\n\n\n\nMar 15, 2022\n\n\nAnaïs Murat, Cathy Roche, Janet Choi, Lavanya Vinod Pampana, Sharmi Dev Gupta, Yanlin Mi, Naa Korkoi Addo, Kislay Raj\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/issue02/index.html#crt-ai-events-diversity-driving-innovation",
    "href": "posts/issue02/index.html#crt-ai-events-diversity-driving-innovation",
    "title": "It’s Time To Talk About Bias",
    "section": "CRT-AI Events: Diversity Driving Innovation",
    "text": "CRT-AI Events: Diversity Driving Innovation\n\nby Naa Korkoi Addo\n\n\n\n\n\n\nAI and Data Science meetup at BNY Mellon\n\n\n\n\nAI and Data Science meetup at BNY Mellon\n25th May, 2022\n   ​The meet up was held in the BNY Mellon offices Dublin on the 25th May 2022. Researchers from third level institutions and guests from industry were in attendance. During the event, the panelists shared their career stories; from where they started to where they currently are. They did not fail to bring in the good, bad, and ugly as the theme of the event was Diversity Driving Innovation. This was a very topical issue and the aim was to increase awareness around women’s participation in the IT sector.\n    ​Dr Suzanne Little opened with a discussion around her career and research to date and mentioned that she had no intention of becoming a lecturer at the start of her career. She also touched upon on how she seized opportunities and how her ambitions and  curiosity positively impacted her life.\n   Joanna Murphy also contributed to her story of how she switched between jobs and ultimately reached her current position. She stressed the fact that it was not always plain sailing. During the meet up, participants had the opportunity to ask questions about how industries are adapting culturally to include more women in their organizations. Dr Little touched upon the fact that just reaching out to other females to encourage them to enter the field is not enough. \n    Eoin Lane shared a story about a female colleague in his department that was performing remarkably well and yet left the industry. He asked the panel and audience what were the factors that could affect women’s decision making when considering career change or moves out of the industry."
  }
]