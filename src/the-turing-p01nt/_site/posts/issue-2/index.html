<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Anaïs Murat">
<meta name="author" content="Cathy Roche">
<meta name="author" content="Janet Choi">
<meta name="author" content="Lavanya Vinod Pampana">
<meta name="author" content="Sharmi Dev Gupta">
<meta name="author" content="Yanlin Mi">
<meta name="author" content="Naa Korkoi Addo">
<meta name="author" content="Kislay Raj">
<meta name="dcterms.date" content="2022-08-15">

<title> - It’s Time To Talk About Bias</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title"><img src="../../logo.svg" height="32" alt="CRT in AI Newsletter"></span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://www.crt-ai.ie/" target="https://www.crt-ai.ie/"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/crt_ai" target="https://twitter.com/crt_ai"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/company/crtai" target="https://www.linkedin.com/company/crtai"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.youtube.com/@crt-aimedia2559" target="https://www.youtube.com/@crt-aimedia2559"> <i class="bi bi-youtube" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:janetchoi@ucc.ie"> <i class="bi bi-envelope-fill" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">It’s Time To Talk About Bias</h1>
            <p class="subtitle lead">THE TURING P01NT | ISSUE #2</p>
                                <div class="quarto-categories">
                <div class="quarto-category">Podcast</div>
                <div class="quarto-category">Dictionary</div>
                <div class="quarto-category">PhD</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Authors</div>
      <div class="quarto-title-meta-contents">
               <p><a href="https://www.crt-ai.ie/team/anais-claire-murat">Anaïs Murat</a> </p>
               <p><a href="https://www.crt-ai.ie/team/cathy-roche">Cathy Roche</a> </p>
               <p><a href="https://www.crt-ai.ie/team/janet-choi">Janet Choi</a> </p>
               <p><a href="https://www.crt-ai.ie/team/lavanya-pampana">Lavanya Vinod Pampana</a> </p>
               <p><a href="https://www.crt-ai.ie/team/sharmi-dev-gupta">Sharmi Dev Gupta</a> </p>
               <p><a href="https://www.crt-ai.ie/team/yanlin-mi">Yanlin Mi</a> </p>
               <p><a href="https://www.crt-ai.ie/team/naa-korkoi">Naa Korkoi Addo</a> </p>
               <p><a href="https://www.linkedin.com/in/kislayraj95/?originalSubdomain=uk">Kislay Raj</a> </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">August 15, 2022</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p><img src="ai-bias.jpg" title="CRT AI Image of AI Bias" id="issue2-img-ai-bias" class="img-fluid" alt="CRT AI Image of AI Bias"></p>
<section id="welcoming-words" class="level3">
<h3 class="anchored" data-anchor-id="welcoming-words">Welcoming words</h3>
<p>Welcome to the second issue of the CRT-AI Newsletter “The Turing Point”. This time we talk about bias in AI. Even though the topic has received quite a lot of attention in recent years, the level of awareness remains low. Knowing what tools a scientist can use to spot, evaluate and mitigate any form of bias is of great importance. For this issue, our team prepared a list of interesting articles, tools and datasets that can help you on your PhD ​​​​​journey.</p>
</section>
<section id="ace-your-phd-life-approaching-bias-in-ai" class="level1">
<h1>Ace Your PhD Life Approaching Bias in AI</h1>
<ul>
<li><p><a href="https://crtai.news/TheTuringPoint-Issue-2.html#tab-0da5"><strong>Problem</strong></a></p></li>
<li><p><a href="https://crtai.news/TheTuringPoint-Issue-2.html#tab-14b7"><strong>Tools</strong></a></p></li>
<li><p><a href="https://crtai.news/TheTuringPoint-Issue-2.html#tab-2917"><strong>Datasets<br>
</strong></a></p></li>
</ul>
<p>Do you want to know how biased your mind is? Try taking The Implicit Association Test (IAT).</p>
<p><a href="https://implicit.harvard.edu/implicit/takeatest.html"><strong>AM I BIASED?</strong></a></p>
<p>1) Scroll down and press “I wish to proceed”<br>
2) Find the “Gender – Science” button<br>
3) Do that test</p>
<p>In recent years, scientific society has started to wrestle with just how much human biases can make their way into artificial intelligence systems—with harmful results.<br>
<br>
“Bias in AI occurs when results cannot be generalized widely. We often think of bias resulting from preferences or exclusions in training data, but bias can also be introduced by how data is obtained, how algorithms are designed, and how AI outputs are interpreted.<br>
How does bias get into AI? Everybody thinks of bias in training data – the data used to develop an algorithm before it is tested on the wide world. But this is only the tip of the iceberg,”&nbsp; says Dr.&nbsp;Sanjiv M. Narayan from Stanford University.<br>
<br>
Our team prepared for you several tools and articles about datasets to explore and work with bias. You can reach them by clicking the tabs “tools” and “datasets”. In order to truly understand where the bias begins, you can try The Implicit Association Test.</p>
</section>
<section id="find-your-crt-ai-science-buddy" class="level1">
<h1>Find Your CRT-AI Science Buddy</h1>
<p>Fancy a collaboration with your CRT-AI mates? You have created that amazing tool but nobody knows about it? You have spent hundreds of hours gathering this incredible database but you are the only one using it? We have your back!<br>
<br>
The CRT-AI’s Padlet now provides you with a nice place to advertise your work and find your AI soulmate &lt;3. Whether you are looking for a collaboration or have a nice database to share, you can publish a post with everything you want people to know. If it isn’t your case yet, remember to save this Padlet to your favorites; you could be surprised by how inspiring this mood board will soon be! Here we welcome every single one of our members and want everyone to feel comfortable. You can therefore post by yourself or by filling in our special form!</p>
<p><a href="https://padlet.com/murata3/kehzurdumkmdsqqr"><strong>FIND YOUR BUDDY</strong></a><a href="https://forms.gle/VzWZx7dLm6Scbxv68">or fill the form and we will do the job</a></p>
<section id="featured-article" class="level3">
<h3 class="anchored" data-anchor-id="featured-article">FEATURED ARTICLE</h3>
</section>
</section>
<section id="the-voice-question-is-ai-voice-gendered" class="level1">
<h1>The Voice Question: Is AI Voice Gendered?</h1>
<section id="by-anaïs-murat" class="level6">
<h6 class="anchored" data-anchor-id="by-anaïs-murat"><em>by ​​Anaïs Murat</em></h6>
<p><img src="./TheTuringPoint Issue 2_files/voice-assistants-transforming-the-lives.jpeg" class="img-fluid" alt=""></p>
<p>Despite all of them being synthetic voices, default assistants’ voices have one thing in common: they sound feminine. Here, we try to investigate why it is so.</p>
</section>
<section id="is-voice-gendered" class="level3">
<h3 class="anchored" data-anchor-id="is-voice-gendered"><strong>Is voice gendered?</strong></h3>
<p>Although these frequencies overlap, women and men tend to use different frequency ranges when they speak. They also have distinct intonation patterns and, more broadly, their speech is tinted with lexical, syntactic and pronunciation differences.</p>
<p><strong>Read more</strong></p>
<p>Research shows that pitch tends to differ: typical male voices are usually comprised between 100 Hz and 200 Hz, while typical female voices are between 120 Hz and 350 Hz [1].&nbsp;</p>
<p>Yet, it is interesting to notice the overlap between these two vocal ranges.</p>
<p>Voices also differ in terms of prosody, i.e the intonation of an utterance follows different patterns depending on the gender of the person (ex: rising intonation at the end of an assertive sentence is more common in women’s speech) [2].</p>
<p>On top of it, speech is not just a voice, and many sociolinguistic works have highlighted the differences in pronunciation, vocabulary, and syntax between women’s and men’s speeches [3].</p>
</section>
<section id="are-these-gender-based-differences-innate-or-learnt" class="level3">
<h3 class="anchored" data-anchor-id="are-these-gender-based-differences-innate-or-learnt"><strong>Are these gender-based differences innate or learnt?</strong></h3>
<p>Tough one. ​While puberty impacts the vocal tract of men and women, language is socially constructed. Proof of this is that by the age of 4, children have already embraced the linguistic form that matches their gender.</p>
<p><strong>Read more</strong></p>
<p>That is the one big question in psycholinguistics, the field that studies language development in humans.</p>
<p>Innate to some extent…</p>
<p>Yes, with puberty, the length of the vocal cords and the shape of the larynx change and grow differently depending on the sex of the individual. These organs determine the fundamental frequency of someone’s speech, thus impacting their pitch (you can read more about this by researching the ‘source-filter theory’ in speech processing) [4].</p>
<p>…but mostly learnt.</p>
<p>Yet, voice is such a big part of someone’s identity that it necessarily matches some social criteria. It can be trained -see the outrageous quantity of Youtube Videos to train a deeper voice- to match social expectations. Not only actors and actresses are often able to use different voices, but even one individual might proceed to a change of pitch depending on the context of the interaction and their intention. Thus, even though the distinction between male and female voices becomes clearer during the teenage years -when one’s body undergoes various changes, the social pressure to fit in the box that teenagers undergo throughout these years is another aspect that should not be overlooked. Another example of the social impact of gender on voices is that children tend to have distinct voices by age 4 despite still having the exact same vocal tract [5].&nbsp;</p>
<p>Finally, and as mentioned in the previous paragraph, all the distinctions in vocabulary and intonation between genders are strong supports of the social aspect of speech.</p>
</section>
<section id="which-gender-is-preferable-in-the-context-of-technology-development" class="level3">
<h3 class="anchored" data-anchor-id="which-gender-is-preferable-in-the-context-of-technology-development"><strong>Which gender is preferable in the context of technology development?</strong></h3>
<p>Female voices are preferred by customers and science overwhelmingly shows that they are more intelligible; even though this has nothing to do with their acoustic properties.</p>
<p><strong>Read more</strong></p>
<p><strong>The social construction of the female voice</strong></p>
<p>Higher pitch is usually associated with youth and femininity. It is perceived as more melodious and uttered by someone calmer, less threatening, and more submissive [6]. To its extreme, yet, it might also sound too shrill. On the other hand, lower pitch &nbsp;associated with masculinity is perceived as more threatening, determined, but also suaver [7].&nbsp;</p>
<p><strong>Technology wise: a recipe for success</strong></p>
<p><strong>1 .Find the scientific support you need</strong></p>
<p>The literature is quite solid on that topic and female voices are overwhelmingly reported as being more intelligible [8], [9]. It is hypothesized that this would be caused by pronunciation differences between female and male rather than their higher pitch. These difference could be overcome by training the assistant to pronounce vowels well.</p>
<p>In fact, higher-pitched voices are even detrimental to people suffering from presbycusis (hearing loss): higher frequencies are the first ones to be affected, making it harder for older people to hear a feminine voice.</p>
<p><strong>2. Please your customers</strong></p>
<p>Yet, the use of stereotypical voices allows a better mental representation of the vocal assistants (AI), which is beneficial to the company: “this is because users prefer simple, easy-to-understand interface, and a stereotype can provide a solid image of computer agent that is more familiar and acceptable to users.” [10].</p>
<p><strong>3. Confirm their biases</strong></p>
<p>Further investigation revealed to what extent the feminine prejudices were embodied within the technology. By simply engaging in a conversation with South Korean vocal assistants (VA), they revealed that the AIs depicted themselves as beautiful women, who would remain nice, available and quiet, even when insulted and sexualised [10].</p>
</section>
<section id="could-a-neutral-voice-be-the-alternative-we-need" class="level3">
<h3 class="anchored" data-anchor-id="could-a-neutral-voice-be-the-alternative-we-need"><strong>Could a neutral voice BE the alternative we need?</strong></h3>
<p>They’ve already done it!<br>
<br>
&nbsp;<a href="https://youtu.be/lvv6zYOQqm0"><strong>LISTEN!</strong></a><br>
<br>
<strong>A Suitable Neutral Voice? Really?<br>
Here, we question the neutrality of this voice, and argue that it might just move the problem somewhere else and reinforce people’s prejudices.&nbsp;​The question of neutrality in linguistics was examined and we reached the following conclusion that we, speakers of a language, have the power to decide what is neutral… but only within our own linguistic community.</strong></p>
<p><strong>Read more</strong></p>
<p>Indeed, people cannot agree on the gender of this voice. In this sense, this voice sounds gender-neutral. Yet, its neutrality might stop just there. Q was developed thanks to the recording of non-binary people. The assumption was that, considering the hugely social aspect of voice their voices were more likely to be non-gendered. Even though this voice does not perfectly fit in the box of female or male, it is not outside of all boxes either. The prosody, intonation and vocabulary may reproduce patterns creating new prejudices. In linguistics neutrality usually just means “standardised’’. What is standard and what people are used to is considered neutral. Many people who listened to Q were confused and unable to get a mental representation of “who” was speaking. This does not necessarily mean that we should give up on this voice… On the contrary, maybe should we make it more prominent and encourage its use, until it becomes a new standard..</p>
<p><strong>A Global Challenge?</strong></p>
<p>In conclusion, all examples discussed were taken from English-speaking voices. The question of cross-language representation is still necessary. Speech frequencies, prosodic contour, phonological systems, and many more aspects of speech are language-specific [3]. Therefore, these “neutral voices” have to be thought of and created for every language, and every accent. Furthermore, if Q’s methodology was adopted by every linguistic community, non-binary people would have to be recruited. There may not be a global answer, and perhaps all we can do is to speak up for our own linguistic community and support what sounds good to us, encouraging more representation and fewer biases in voices.&nbsp;</p>
</section>
<section id="bibliography" class="level3">
<h3 class="anchored" data-anchor-id="bibliography"><strong>Bibliography</strong></h3>
<p><strong>Open it</strong></p>
<p>[1] &nbsp; &nbsp;C. Pernet and P. Belin, “The Role of Pitch and Timbre in Voice Gender Categorization,” Front. Psychol., vol.&nbsp;3, 2012, Accessed: Jun.&nbsp;17, 2022. [Online]. Available: https://www.frontiersin.org/article/10.3389/fpsyg.2012.00023</p>
<p>[2] &nbsp; &nbsp;Anthony Pym, Do women and men use language the same way?, (Jan.&nbsp;25, 2019). Accessed: Jun.&nbsp;16, 2022. [Online Video]. Available: https://www.youtube.com/watch?v=Txd93vZQHWU</p>
<p>[3] &nbsp; &nbsp;J. Holmes and N. Wilson, An introduction to sociolinguistics, Fifth Edition. London ; New York, NY: Routledge, 2017.</p>
<p>[4] &nbsp; &nbsp;L. Véron, “La voix neutre n’existe pas.” Accessed: Jun.&nbsp;17, 2022. [Online]. Available: https://www.binge.audio/podcast/parler-comme-jamais/la-voix-neutre-nexiste-pas/</p>
<p>[5] &nbsp; &nbsp;H. K. Vorperian and R. D. Kent, “Vowel Acoustic Space Development in Children: A Synthesis of Acoustic and Anatomic Data,” J. Speech Lang. Hear. Res. JSLHR, vol.&nbsp;50, no. 6, pp.&nbsp;1510–1545, Dec.&nbsp;2007, doi: 10.1044/1092-4388(2007/104).</p>
<p>[6] &nbsp; &nbsp;A. Arnold and M. Candea, “Comment étudier l’influence des stéréotypes de genre et de race sur la perception de la parole ?,” Lang. Société, vol.&nbsp;152, no. 2, pp.&nbsp;75–96, 2015, doi: 10.3917/ls.152.0075.</p>
<p>[7] &nbsp; &nbsp;A. Arnold, “La voix genrée, entre idéologies et pratiques – Une étude sociophonétique,” p.&nbsp;311.</p>
<p>[8] &nbsp; &nbsp;A. R. Bradlow, G. M. Torretta, and D. B. Pisoni, “Intelligibility of normal speech I: Global and fine-grained acoustic-phonetic talker characteristics,” Speech Commun., vol.&nbsp;20, no. 3, pp.&nbsp;255–272, Dec.&nbsp;1996, doi: 10.1016/S0167-6393(96)00063-5.</p>
<p>[9] &nbsp; &nbsp;H.-B. Kwon, “Gender difference in speech intelligibility using speech intelligibility tests and acoustic analyses,” J. Adv. Prosthodont., vol.&nbsp;2, no. 3, pp.&nbsp;71–76, Sep.&nbsp;2010, doi: 10.4047/jap.2010.2.3.71.</p>
<p>[10] &nbsp; &nbsp;G. Hwang, J. Lee, C. Y. Oh, and J. Lee, “It Sounds Like A Woman: Exploring Gender Stereotypes in South Korean Voice Assistants,” in Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems, New York, NY, USA, May 2019, pp.&nbsp;1–6. doi: 10.1145/3290607.3312915.</p>
</section>
</section>
<section id="turing-point-podcast" class="level1">
<h1>Turing Point Podcast</h1>
<p>Episode 2 of the Turing Point Podcast! CRT-AI PhD students Anais Murat and Sharmi Dev Gupta moderate a lively conversation with Dr.&nbsp;Begum Genc and CRT-AI student Buvana Ganesh about the impacts of gender bias in Artificial Intelligence.</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/nda9QT8-PEY?start=116" title="Turing Point Podcast" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</section>
<section id="artificial-intelligence-dictionary" class="level1">
<h1><strong>A</strong>rtificial <strong>I</strong>ntelligence <strong>D</strong>ictionary</h1>
<section id="coming-to-your-aid-ai-dictionary" class="level4">
<h4 class="anchored" data-anchor-id="coming-to-your-aid-ai-dictionary">Coming to your AID (AI Dictionary)</h4>
<section id="by-cathy-roche" class="level6">
<h6 class="anchored" data-anchor-id="by-cathy-roche"><em>by ​Cathy Roche</em></h6>
<ol type="1">
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
</ol>
</section>
</section>
<section id="data-feminism" class="level3">
<h3 class="anchored" data-anchor-id="data-feminism"><strong>Data Feminism</strong></h3>
<section id="ˈdeɪtə-ˈfɛmɪnɪzəm" class="level5">
<h5 class="anchored" data-anchor-id="ˈdeɪtə-ˈfɛmɪnɪzəm">/ˈdeɪtə/ /ˈfɛmɪnɪz(ə)m/</h5>
<p>Data feminism is a framework for thinking about data, informed by intersectional feminist thought. It is characterised by a focus on direct experience and by a commitment to action. Data feminism acknowledges that power is not equally distributed in the world and that data is a form of power for those who have the resources to collect and analyse it. It uncovers how standard practices in data science serve to reinforce existing inequalities, such as in medical care and hiring processes. However, data feminism is also a call to use data science to challenge and change the distribution of power. It questions binary and hierarchical classification systems in order to promote equitable and gender-sensitive data systems. Data feminism isn’t only about women, isn’t only for women and isn’t only about gender. It is about power and working towards a society where everyone benefits equitably from data and technology.</p>
</section>
</section>
<section id="crt-ai-events-diversity-driving-innovation" class="level2">
<h2 class="anchored" data-anchor-id="crt-ai-events-diversity-driving-innovation">CRT-AI Events:&nbsp;Diversity Driving Innovation</h2>
<section id="by-naa-korkoi-addo" class="level4">
<h4 class="anchored" data-anchor-id="by-naa-korkoi-addo">by Naa Korkoi Addo</h4>
<ol type="1">
<li></li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./TheTuringPoint Issue 2_files/1653515533090.jpeg" class="img-fluid figure-img"></p>
<figcaption>AI and Data Science meetup at BNY Mellon</figcaption>
</figure>
</div>
</section>
<section id="ai-and-data-science-meetup-at-bny-mellon" class="level3">
<h3 class="anchored" data-anchor-id="ai-and-data-science-meetup-at-bny-mellon">AI and Data Science meetup at BNY Mellon</h3>
<p>25th May, 2022</p>
<p>&nbsp; &nbsp;​The meet up was held in the BNY Mellon offices Dublin on the 25th May 2022. Researchers from third level institutions and guests from industry were in attendance. During the event, the panelists shared their career stories; from where they started to where they currently are. They did not fail to bring in the good, bad, and ugly as the theme of the event was Diversity Driving Innovation. This was a very topical issue and the aim was to increase awareness around women’s participation in the IT sector.<br>
&nbsp; &nbsp; ​Dr Suzanne Little opened with a discussion around her career and research to date and mentioned that she had no intention of becoming a lecturer at the start of her career. She also touched upon on how she seized opportunities and how her ambitions and&nbsp; curiosity positively impacted her life.<br>
&nbsp;&nbsp;&nbsp;Joanna Murphy also contributed to her story of how she switched between jobs and ultimately reached her current position. She stressed the fact that it was not always plain sailing. During the meet up, participants had the opportunity to ask questions about how industries are adapting culturally to include more women in their organizations. Dr Little touched upon the fact that just reaching out to other females to encourage them to enter the field is not enough.&nbsp;<br>
&nbsp; &nbsp; Eoin Lane shared a story about a female colleague in his department that was performing remarkably well and yet left the industry. He asked the panel and audience what were the factors that could affect women’s decision making when considering career change or moves out of the industry.<br>
&nbsp; &nbsp; &nbsp;&nbsp;</p>
</section>
</section>
</section>
<section id="gender-bias-in-automated-speech-recognition" class="level1">
<h1>Gender Bias in Automated Speech Recognition</h1>
<section id="by-kislay-raj-and-cathy-roche" class="level6">
<h6 class="anchored" data-anchor-id="by-kislay-raj-and-cathy-roche">by Kislay Raj and Cathy Roche</h6>
<p>&nbsp; &nbsp;&nbsp; <strong>Dating back to 1878, Emma Nutt was considered the first woman telephone operator. Her voice was so well received that she soon became the standard for all other companies. By the 1880s, all telephone operators were women. We now have over a hundred years of female voice recordings that can be used to create new automated voice assistants. Gender bias in voice automation reflects this lack of male voice data as well as accepted assumptions about the female voice [1].</strong><br>
&nbsp; &nbsp;&nbsp; <strong>Developers rely on female voices because of the lack of an equivalent male voice training set [<a href="https://www.adaptworldwide.com/insights/2021/gender-bias-in-ai-why-voice-assistants-are-female">1</a>]. As a result, the first speech assistant was made using a female voice, as it was easier to use existing data than develop a new dataset. Furthermore, even if a new male voice dataset were created, there is a risk that it might not be as well received as the female version. It was simply more expedient for companies to use existing data known to have general public acceptance, even if this meant perpetuating gender bias [1].<br>
&nbsp; &nbsp; &nbsp; In Machine Learning (ML), a model makes predictions based on the data provided as a training set. Natural Language Processing (NLP) has enabled ML models to recognise the gender of voices. If the training data is imbalanced and uses more samples from one class​, then there will be a bias towards that class. The model can make more accurate predictions for the data it has seen most frequently [2].</strong></p>
<p>As humans, we may assume that when we communicate, we do so without bias towards a particular group. We interpret different languages and perform specific tasks based on making meaning from the language. When machines are tasked with replicating this process, computers ‘recognise’ speech, and then the natural language unit performs the ‘understanding’ of words, referred to as interpreters [2]. This process is one of AI’s most complex tasks, as the system attempts to generate output that is as ‘natural’ as human speech. Gender bias appears because of various ambiguities in ML models, including lexical ambiguity. As AI bots might be unable to ‘recognise’ words correctly, they perform predictions based on limited data sets. Any bias in the dataset will be reflected in the predictions made. Recognition errors can also result from pragmatic ambiguity, which gives different meanings to different words and sentences, depending on context [4].</p>
<p>&nbsp; &nbsp; Researchers at Stanford University found that individuals deal with machines in the same manner they treat humans. A lack of diversity amongst developers has resulted in algorithms with significant bias in AI models, as the models do not reflect the broader population [5]. &nbsp;Due to the nature of the data, AI bots replicate and can reinforce gender assumptions built into the original data, for example, the association of female-voiced assistants with a submissive and malleable nature [5].</p>
<p><strong>How to Make Automatic Speech Recognition (ASR) Systems Less Biased?</strong></p>
<p>&nbsp; &nbsp; Due to the complexity and richness of human speech, it is unrealistic to believe that bias (in the ML sense) will ever be eliminated completely from Automated Speech Recognition (ASR) systems. It is reasonable to think that it will take time for improvements in AI and ML. After all, as humans, we occasionally have difficulty comprehending speakers of other languages or those with different accents.</p>
<p>&nbsp; &nbsp; What can be done when it is clear that a model is not working well for a particular set of people? There are several approaches to help improve model performance.&nbsp;</p>
<p><strong>Proposed solution</strong></p>
<p>&nbsp; &nbsp; One solution involves examining the whole ML pipeline: (i) Dataset, (ii) Training (or the model) and (iii) Results. &nbsp;Within the dataset, ensuring a balanced distribution of all subgroups can be achieved when pre-processing the data. During training, one can include the constraints of fairness like demographic parity (statistical independence between outcome and demographics), so that the model optimises for accuracy and fairness. Finally, one can make post-hoc changes to the outputs so that the demographic distribution is balanced. The figure below indicates techniques that can be used to address bias at each stage [6,7].</p>
<p><img src="./TheTuringPoint Issue 2_files/pic1.png" class="img-fluid"></p>
<p><strong>Conclusion</strong></p>
<p>&nbsp; &nbsp; Within ML, biased data produces biased models: whether it is intentional or unintentional, any data not reflective of the range of potential outcomes, will result in bias. Sampling bias can lead to inaccurate models, particularly if they are built using historical datasets which have built-in biases. For example, if a company trains a model to assist in decision-making about promotions and has a poor track record of promoting women, its model will likely make the same biased decisions because of the nature of the training data. Similarly, a model trained on speech that was simple for its inventor to collect (from themselves, their family and friends), who might all speak with similar accents or inflection, the resulting ASR model may reflect a preference for such voices and may not recognise those with a different tone or accent [7].</p>
<p>&nbsp; &nbsp; While there is no easy, universal method for identifying and addressing bias in ASR systems, it is important that all data is examined for potential bias before models are developed and deployed. Observe patterns in the data, anticipate the populations affected by the model’s decisions and be aware of what is missing from the dataset.&nbsp;</p>
<p><strong>Bibliography</strong></p>
<p>[1] &nbsp;Adapt. 2022. Gender Bias in AI: Why Voice Assistants Are Female | Adapt - Adapt. [online] Available at: [Accessed 22 May 2022].</p>
<p>[2] Robison, C., 2022. How AI bots and voice assistants reinforce gender bias. [online] Brookings. Available at: [Accessed 20 May 2022].</p>
<p>[3] Harvard Business Review. 2022. Voice Recognition Still Has Significant Race and Gender Biases. [online] Available at: &nbsp;[Accessed 20 May 2022].</p>
<p>[4] L. Zhang, Y. Wu, and X. Wu, “A causal framework for discovering and removing direct and indirect discrimination,” CoRR, vol.&nbsp;abs/1611.07509, 2016</p>
<p>[5] Female IBM Researchers are helping AI overcome bias and find its voice | IBM Research Blog. [online] IBM Research Blog. Available at: &nbsp;[Accessed 25 May 2022].</p>
<p>[6] Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K. and Galstyan, A., 2021. A survey on bias and fairness in machine learning. ACM Computing Surveys (CSUR), 54(6), pp.1-35.</p>
<p>[7] Yuan, M., Kumar, V., Ahmad, M.A.&nbsp;and Teredesai, A., 2021. Assessing Fairness in Classification Parity of Machine Learning Models in Healthcare. arXiv preprint arXiv:2102.03717.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/crt-ai\.quarto\.pub\/newsletter-turing-point");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>